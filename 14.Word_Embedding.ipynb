{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding\n",
    "\n",
    "**Word embedding** (или векторное представление слов) - техника в NLP, когда с помощью unsupervised (чаще всего) алгоритма слову или фразе из словаря сопоставляется вещественный вектор фиксированной размерности. Этот вектор (чаще всего) тем или иным образом характеризует семантику слова. Вектор может зависить от контекста слова (и быть чувствительным к омонимии), а может и не зависить. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "\n",
    "[Word2vec](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) - один из первый метод word embedding ставших популярным. \n",
    "\n",
    "Два подхода:\n",
    "- Continues Bag of Words (CBOW)\n",
    "- Skip gram\n",
    "\n",
    "![Источник - https://arxiv.org/pdf/1301.3781.pdf ](images/w2v.png)\n",
    "\n",
    "Вероятность слова в контексте можно интерпретировать как:\n",
    "\n",
    "$$p(w_o|w_I) = \\frac{\\exp(v′_{w_O}^T \\cdot v_{w_I})}{\\sum_{w=1}^{W}\\exp(v′_{w_i}^T \\cdot v_{w_I})}$$\n",
    "\n",
    "Для оптимизации используется или negative sampling или hierarchical softmax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastText\n",
    "\n",
    "[fastText](https://fasttext.cc/) - библиотека от Facebook.\n",
    "- большое количество предтренированныйх моделей для разных языков\n",
    "- может сопоставлять вектора для слов вне словаря\n",
    "\n",
    "Принцип работы - слово делится на N-граммы. Каждой N-грамме сопоставляется вектор, для получения векторного представления всего слова вектора N-грамм суммируются. \n",
    "\n",
    "**привет** $\\rightarrow$ **<при**, **при**, **рив**, **иве**, **вет**, **вет>**. \n",
    "\n",
    "Дальше идея таже. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GloVe\n",
    "\n",
    "[GloVe](https://nlp.stanford.edu/pubs/glove.pdf) - имплементация через минимизацию функционала:\n",
    "\n",
    "$$J = \\sum_{i,j=1}^V f(X_{ij}) (w_i \\cdot \\tilde{w_j} + b_i + \\tilde{b_j} - \\log X_{ij})$$\n",
    "\n",
    "где, $X_{ij}$ - матрица взаимовстречаемости слов (сколько раз слово $i$ имело в контексте слово $j$). \n",
    "\n",
    "$f(x)= \\begin{cases}\n",
    "    (\\frac{x}{x_{max}})^\\alpha, & \\text{если $x<x_{max}$}.\\\\\n",
    "    1, & \\text{иначе}.\n",
    "  \\end{cases}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
